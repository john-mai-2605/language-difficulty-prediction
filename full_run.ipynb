{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "full_run",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOGmfoDHtLFE3BeUayyMfpo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/john-mai-2605/language-difficulty-prediction/blob/master/full_run.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9UbnHdwFa3i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "5cb134b6-dc70-4190-950a-75b1b0c34602"
      },
      "source": [
        "!pip install cmudict\n",
        "!pip install imblearn"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting cmudict\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/cf/4d24ac4f3ea5a57406a690ad7c07023c310185eac99adae7473c9ebdf550/cmudict-0.4.4-py2.py3-none-any.whl (938kB)\n",
            "\u001b[K     |████████████████████████████████| 942kB 3.4MB/s \n",
            "\u001b[?25hInstalling collected packages: cmudict\n",
            "Successfully installed cmudict-0.4.4\n",
            "Requirement already satisfied: imblearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.6/dist-packages (from imblearn) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn->imblearn) (1.18.5)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn->imblearn) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn->imblearn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20->imbalanced-learn->imblearn) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcPC_F2XFM91",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "7a67a653-1616-4bea-b305-a8f4b9a00029"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import cmudict\n",
        "from sklearn import *\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import pickle\n",
        "from sklearn.externals.joblib import parallel_backend\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "from sklearn.preprocessing import RobustScaler, OrdinalEncoder, OneHotEncoder, MinMaxScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.linear_model import ElasticNet, Lasso, BayesianRidge, LassoLarsIC\n",
        "from sklearn.linear_model import LinearRegression, SGDRegressor, PassiveAggressiveRegressor, HuberRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "\n",
        "import xgboost as xgb \n",
        "import lightgbm as lgb"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTm1SaILFqcU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# clustering\n",
        "n_clusters = 35\n",
        "tag_cluster_error_rate = 'tag_cluster_error_rate'\n",
        "tag_clustering_features = [\n",
        "    'action', 'adventure', 'american', 'animal',\n",
        "    'animation', 'australian', 'british', 'comedy',\n",
        "    'cooking', 'documentary', 'drama', 'education',\n",
        "    'english', 'fantasy', 'food', 'foreign accent',\n",
        "    'interview', 'monologue', 'movie', 'news',\n",
        "    'review', 'romance', 'sciencefiction', 'sitcom',\n",
        "    'song', 'speech', 'superhero', 'tvseries',\n",
        "    'talkshow', 'technology', 'thriller', 'trailer',\n",
        "]\n",
        "tag_cluster = 'tag_cluster'\n",
        "corr_cluster_error_rate = 'corr_cluster_error_rate'\n",
        "corr_clustering_features = ['elapse_time', 'speed', 'wpm', 'aveAmbiguity']\n",
        "corr_cluster = 'corr_cluster'\n",
        "\n",
        "feature_columns = ['length',\n",
        "            'aveLength', 'maxLength', 'minLength',\n",
        "            'aveFreq', 'maxFreq', 'minFreq', \n",
        "            'aveDepth', 'maxDepth', 'minDepth', \n",
        "            'aveDensity', 'minDensity', 'maxDensity',\n",
        "            'aveAmbiguity', 'minAmbiguity', 'maxAmbiguity', \n",
        "            'wpm', 'elapse_time', 'speed',\n",
        "            'noun', 'verb', 'adj', 'adv',\n",
        "            'det', 'prep', 'norm',\n",
        "            'action', 'adventure', 'american', 'animal',\n",
        "            'animation', 'australian', 'british', 'comedy',\n",
        "            'cooking', 'documentary', 'drama', 'education',\n",
        "            'english', 'fantasy', 'food', 'foreign accent',\n",
        "            'interview', 'monologue', 'movie', 'news',\n",
        "            'review', 'romance', 'sciencefiction', 'sitcom',\n",
        "            'song', 'speech', 'superhero', 'tvseries',\n",
        "            'talkshow', 'technology', 'thriller', 'trailer',\n",
        "            tag_cluster_error_rate, corr_cluster_error_rate,\n",
        "            ]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ROte3z9GABp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dataset:\n",
        "    def __init__(self, path):\n",
        "        self.data = pd.read_csv(path)\n",
        "        # self.X = self.data[schema]\n",
        "        self.y = self.data['error_rate']\n",
        "        self.scaler = RobustScaler()\n",
        "        self.imputer = SimpleImputer(missing_values = np.nan, strategy = 'mean')\n",
        "        self.enc_oh = OneHotEncoder()\n",
        "        self.enc_ord = OrdinalEncoder()\n",
        "        self.kmeans = KMeans(n_clusters=n_clusters, random_state=123)\n",
        "    def split(self, test_ratio):\n",
        "        return model_selection.train_test_split(self.data, self.y, test_size = test_ratio, random_state=42)\n",
        "    \n",
        "    def encode(self):\n",
        "        pass\n",
        "        # self.X['tag'] = self.enc_ord.fit_transform(self.X['tag'].to_numpy().reshape(-1, 1))\n",
        "        # self.X['genreTag'] = self.enc_ord.fit_transform(self.X['genreTag'].to_numpy().reshape(-1, 1))\n",
        "\n",
        "    def normalize(self, X_train, X_test):\n",
        "        self.imputer.fit(X_train)\n",
        "        X_train = self.imputer.transform(X_train)\n",
        "        X_test = self.imputer.transform(X_test)\n",
        "        self.scaler.fit(X_train) \n",
        "        return self.scaler.transform(X_train), self.scaler.transform(X_test)\n",
        "    def select(self, X_train, y_train, X_test, k = 20):\n",
        "        self.selector = SelectKBest(chi2, k)\n",
        "        self.selector.fit(X_train, y_train)\n",
        "        return self.selector.transform(X_train), self.selector.transform(X_test)\n",
        "    def add_clustering_features(self, train, test, clustering_features, cluster_feature_name, normalize=False):\n",
        "        clustering_X_train = train[clustering_features]\n",
        "        clustering_X_test = test[clustering_features]\n",
        "        if (normalize == True):\n",
        "            scaler = MinMaxScaler()\n",
        "            scaler.fit(clustering_X_train)\n",
        "            train_for_clustering = scaler.transform(clustering_X_train)\n",
        "            test_for_clustering = scaler.transform(clustering_X_test)\n",
        "        # duplicate code to impute before clustering\n",
        "        self.imputer.fit(clustering_X_train)\n",
        "        clustering_X_train = self.imputer.transform(clustering_X_train)\n",
        "        clustering_X_test = self.imputer.transform(clustering_X_test)\n",
        "        #####\n",
        "        self.kmeans.fit(clustering_X_train)\n",
        "        train_labels = self.kmeans.predict(clustering_X_train)\n",
        "        test_labels = self.kmeans.predict(clustering_X_test)\n",
        "        train['cluster'] = train_labels\n",
        "        test['cluster'] = test_labels\n",
        "        cluster_to_median = {}\n",
        "        for i in range(n_clusters):\n",
        "            cluster_to_median[i] = train['error_rate'].loc[train['cluster'] == i].median()\n",
        "        train[cluster_feature_name] = train['cluster'].apply(lambda c:cluster_to_median[c])\n",
        "        test[cluster_feature_name] = test['cluster'].apply(lambda c:cluster_to_median[c])\n",
        "        return [train, test]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVPj9mngGBMa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Regressor:\n",
        "    def __init__(self, reg):\n",
        "        self.reg = reg\n",
        "    def fit(self, X, y, **kwargs):\n",
        "        return self.reg.fit(X, y, **kwargs)\n",
        "    def predict(self, X_test):\n",
        "        return self.reg.predict(X_test)\n",
        "    def evaluate(self, y_test, y_pred): \n",
        "        mse = metrics.mean_squared_error(y_test, y_pred)\n",
        "        r2 = metrics.r2_score(y_test, y_pred)\n",
        "        print(np.sqrt(mse), r2)\n",
        "        clf_test = [map_to_class(y) for y in y_test]\n",
        "        clf_pred = [map_to_class(y) for y in y_pred]\n",
        "        print(metrics.confusion_matrix(clf_test, clf_pred))\n",
        "        print(metrics.classification_report(clf_test, clf_pred))\n",
        "    def tune(self, X_train, Y_train, param_grid,\n",
        "             n_folds = 10, result_filename = 'reg_tuning_results.csv'):\n",
        "        grid_cv = GridSearchCV(estimator = self.reg,\n",
        "                           param_grid = param_grid,\n",
        "                           cv = n_folds,\n",
        "                           scoring = 'neg_mean_squared_error',\n",
        "                           verbose = 1,\n",
        "                           n_jobs = -1\n",
        "                          )\n",
        "        grid_cv.fit(X_train, Y_train)\n",
        "        # save results\n",
        "        tuning_results = pd.DataFrame(grid_cv.cv_results_)\n",
        "        tuning_results.to_csv(result_filename)\n",
        "\n",
        "        # set best params\n",
        "        best_params = grid_cv.best_params_\n",
        "        self.reg.set_params(**best_params)\n",
        "\n",
        "    def run(self, X, y, X_test, y_test, **kwargs):\n",
        "        fitted_model = self.fit(X, y, **kwargs)\n",
        "        y_pred = self.predict(X_test)\n",
        "        self.evaluate(y_test, y_pred)\n",
        "        try:\n",
        "            importance = list(zip(fitted_model.feature_importances_, feature_columns))\n",
        "            importance.sort(reverse=True)\n",
        "            print(importance)\n",
        "        except:\n",
        "            pass\n",
        "        return y_pred\n",
        "    def select(self, X_train, X_test, k = 50):\n",
        "        self.selector = SelectFromModel(self.reg, prefit = True, threshold = -np.inf, max_features = k)\n",
        "        return self.selector.transform(X_train), self.selector.transform(X_test)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6274p3aGJdf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def map_to_class(score, s1 = 2/3, s2 = 1/3):\n",
        "    if score > s1:\n",
        "        return 2\n",
        "    if score > s2:\n",
        "        return 1\n",
        "    return 0"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ohe3fQMAGM4R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5ed072a3-4580-48cf-9e60-aa32aa983fed"
      },
      "source": [
        "data = Dataset('./processed_data.csv')\n",
        "# data.encode()\n",
        "# data.selection()\n",
        "train, test, y_train, y_test = data.split(0.2)\n",
        "# add tag clustering feature\n",
        "train, test = data.add_clustering_features(train, test, tag_clustering_features, tag_cluster_error_rate)\n",
        "# add correlation based clustering feature\n",
        "train, test = data.add_clustering_features(train, test, corr_clustering_features, corr_cluster_error_rate, normalize = True)\n",
        "\n",
        "X_train = train[feature_columns]\n",
        "X_test = test[feature_columns]\n",
        "print(X_train[tag_cluster_error_rate].describe())\n",
        "print(X_train[corr_cluster_error_rate].describe())\n",
        "print(X_test[tag_cluster_error_rate].describe())\n",
        "print(X_test[corr_cluster_error_rate].describe())\n",
        "print('features', feature_columns)\n",
        "print('train size', X_train.shape)\n",
        "print('test size', X_test.shape)\n",
        "X_train, X_test = data.normalize(X_train, X_test)\n",
        "y_train_clf = [map_to_class(y) for y in y_train]\n",
        "y_test_clf = [map_to_class(y) for y in y_test]\n",
        "\n",
        "#X_train, X_test = data.select(X_train, y_train, X_test, 20)\n",
        "\n",
        "X = np.insert(X_train, 0, y_train, axis=1)\n",
        "smote = SMOTE(random_state=27)\n",
        "y_train_clf = [map_to_class(y) for y in y_train]\n",
        "X_new, _ = smote.fit_resample(X, y_train_clf)\n",
        "X_train, y_train = X_new[:, 1:], X_new[:,[0]].flatten()\n",
        "\n",
        "y_train_clf = [map_to_class(y) for y in y_train]\n",
        "y_test_clf = [map_to_class(y) for y in y_test]\n",
        "print(sorted(Counter(y_train_clf).items()))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:45: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:50: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:45: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:50: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "count    19386.000000\n",
            "mean         0.354542\n",
            "std          0.082364\n",
            "min          0.144429\n",
            "25%          0.283871\n",
            "50%          0.339087\n",
            "75%          0.431692\n",
            "max          0.587882\n",
            "Name: tag_cluster_error_rate, dtype: float64\n",
            "count    19386.000000\n",
            "mean         0.360312\n",
            "std          0.184167\n",
            "min          0.010328\n",
            "25%          0.196970\n",
            "50%          0.377674\n",
            "75%          0.532843\n",
            "max          0.640907\n",
            "Name: corr_cluster_error_rate, dtype: float64\n",
            "count    4847.000000\n",
            "mean        0.354840\n",
            "std         0.082620\n",
            "min         0.144429\n",
            "25%         0.283871\n",
            "50%         0.339087\n",
            "75%         0.431692\n",
            "max         0.587882\n",
            "Name: tag_cluster_error_rate, dtype: float64\n",
            "count    4847.000000\n",
            "mean        0.361704\n",
            "std         0.183579\n",
            "min         0.010328\n",
            "25%         0.196970\n",
            "50%         0.377674\n",
            "75%         0.532843\n",
            "max         0.640907\n",
            "Name: corr_cluster_error_rate, dtype: float64\n",
            "features ['length', 'aveLength', 'maxLength', 'minLength', 'aveFreq', 'maxFreq', 'minFreq', 'aveDepth', 'maxDepth', 'minDepth', 'aveDensity', 'minDensity', 'maxDensity', 'aveAmbiguity', 'minAmbiguity', 'maxAmbiguity', 'wpm', 'elapse_time', 'speed', 'noun', 'verb', 'adj', 'adv', 'det', 'prep', 'norm', 'action', 'adventure', 'american', 'animal', 'animation', 'australian', 'british', 'comedy', 'cooking', 'documentary', 'drama', 'education', 'english', 'fantasy', 'food', 'foreign accent', 'interview', 'monologue', 'movie', 'news', 'review', 'romance', 'sciencefiction', 'sitcom', 'song', 'speech', 'superhero', 'tvseries', 'talkshow', 'technology', 'thriller', 'trailer', 'tag_cluster_error_rate', 'corr_cluster_error_rate']\n",
            "train size (19386, 60)\n",
            "test size (4847, 60)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[(0, 9165), (1, 9165), (2, 9165)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7i2dh1pHLXl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "outputId": "8ddf371e-09d8-47c6-c3f0-9a593f1d052e"
      },
      "source": [
        "models = [\n",
        "        ElasticNet(), Lasso(), BayesianRidge(),\n",
        "        LassoLarsIC(), RandomForestRegressor(), HuberRegressor(), \n",
        "        svm.SVR(), neural_network.MLPRegressor(), LinearRegression(), \n",
        "        SGDRegressor(), PassiveAggressiveRegressor(), \n",
        "        xgb.XGBRegressor(), lgb.LGBMRegressor(), GradientBoostingRegressor()\n",
        "        ]\n",
        "EN_param_grid = {'alpha': [0.001, 0.01, 0.0001], 'copy_X': [True], 'l1_ratio': [0.6, 0.7], 'fit_intercept': [True], 'normalize': [False], \n",
        "                         'precompute': [False], 'max_iter': [300, 3000], 'tol': [0.001], 'selection': ['random', 'cyclic'], 'random_state': [None]}\n",
        "LASS_param_grid = {'alpha': [0.001, 0.0001, 0.00001, 0.000001], 'copy_X': [True], 'fit_intercept': [True], 'normalize': [False], 'precompute': [False], \n",
        "                    'max_iter': [3000], 'tol': [0.1, 0.01, 0.001], 'selection': ['random'], 'random_state': [42]}\n",
        "GB_param_grid = {'loss': ['huber'], 'learning_rate': [0.1, 0.01, 0.001], 'n_estimators': [3000], 'max_depth': [3, 10], \n",
        "                                        'min_samples_split': [0.0025], 'min_samples_leaf': [5]}\n",
        "BR_param_grid = {'n_iter': [200, 1000], 'tol': [0.00001, 0.0001], 'alpha_1': [0.00000001, 0.00000005], 'alpha_2': [0.000005, 0.00001], 'lambda_1': [0.000005, 0.00001], \n",
        "                 'lambda_2': [0.00000001, 0.00000005], 'copy_X': [True]}\n",
        "LL_param_grid = {'criterion': ['aic'], 'normalize': [True], 'max_iter': [100, 1000], 'copy_X': [True], 'precompute': ['auto'], 'eps': [0.000001, 0.00001, 0.0001]}\n",
        "RFR_param_grid = {'n_estimators': [50, 500], 'max_features': ['auto'], 'max_depth': [None], 'min_samples_split': [5], 'min_samples_leaf': [2]}\n",
        "XGB_param_grid = {'learning_rate': [0.1], 'n_estimators': [10000]}\n",
        "LGB_param_grid = {'objective': ['regression'], 'learning_rate': [0.05, 0.1, 0.5], 'n_estimators': [300, 3000]}\n",
        "SVR_param_grid = {'kernel': ['rbf']} #['linear', 'poly', 'rbf', 'sigmoid']}\n",
        "MLP_param_grid = {'hidden_layer_sizes': [(100,), (100, 10)], 'random_state': [42], 'max_iter': [100, 1000],  'alpha': [0.01, 0.001, 0.0001]}\n",
        "LR_param_grid = {}\n",
        "GDR_param_grid = {\n",
        "                'max_iter': 5000,\n",
        "                'loss': ['squared_loss'],\n",
        "                'penalty': ['l2', 'elasticnet', 'l1'],\n",
        "                'l1_ratio': [0.7, 0.3],\n",
        "                'learning_rate': ['optimal'],\n",
        "                'alpha': [1e-01, 1e-2],\n",
        "                'epsilon': [1e-01],\n",
        "                'tol': [0.001, 0.003],\n",
        "                'eta0': [0.01],\n",
        "                'power_t': [0.5]}\n",
        "PAR_param_grid = {'whiten': [True, False], 'loss': ['squared_epsilon_insensitive', 'epsilon_insensitive'], \n",
        "                  'C': [0.001, 0.005, 0.003], 'max_iter': [1000], 'epsilon': [0.00001, 0.00005],\n",
        "                  'tol': [1e-03, 1e-05]}\n",
        "HR_param_grid = {'max_iter': [2000], 'alpha': [0.0001, 1e-05,], \n",
        "               'tol': [1e-01, 1e-02]}\n",
        "params_grids = [\n",
        "                EN_param_grid, LASS_param_grid, BR_param_grid, \n",
        "                LL_param_grid, RFR_param_grid, HR_param_grid,\n",
        "                SVR_param_grid, MLP_param_grid, LR_param_grid, \n",
        "                GDR_param_grid, PAR_param_grid, \n",
        "                XGB_param_grid, LGB_param_grid, GB_param_grid\n",
        "                ]\n",
        "regs = []\n",
        "params = []\n",
        "\n",
        "for model, param_grid in zip(models, params_grids):\n",
        "    print(model.__class__.__name__)\n",
        "    regressor = Regressor(model)\n",
        "    print('Start tuning')\n",
        "    with parallel_backend('threading'):\n",
        "      regressor.tune(X_train, y_train, param_grid, result_filename = model.__class__.__name__ + ' hyperparameters.csv')\n",
        "    print('Finish tuning')\n",
        "    params.append(regressor.reg.get_params())\n",
        "    y = regressor.run(X_train, y_train, X_test, y_test)\n",
        "    regs.append(regressor)\n",
        "\n",
        "with open('regs.pkl', 'wb') as f:\n",
        "    pickle.dump(regs, f)\n",
        "with open('params.pkl', 'wb') as f:\n",
        "    pickle.dump(params, f)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ElasticNet\n",
            "Start tuning\n",
            "Fitting 10 folds for each of 24 candidates, totalling 240 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:   13.5s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:   57.4s\n",
            "[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed:  1.3min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finish tuning\n",
            "0.1481723513869192 0.6793531217693163\n",
            "[[1619  640   16]\n",
            " [ 162 1383  171]\n",
            " [   9  391  456]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.71      0.80      2275\n",
            "           1       0.57      0.81      0.67      1716\n",
            "           2       0.71      0.53      0.61       856\n",
            "\n",
            "    accuracy                           0.71      4847\n",
            "   macro avg       0.73      0.68      0.69      4847\n",
            "weighted avg       0.75      0.71      0.72      4847\n",
            "\n",
            "Lasso\n",
            "Start tuning\n",
            "Fitting 10 folds for each of 12 candidates, totalling 120 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:   11.1s\n",
            "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed:  1.5min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Finish tuning\n",
            "0.14814994905475032 0.6794500723108605\n",
            "[[1626  634   15]\n",
            " [ 164 1379  173]\n",
            " [   9  393  454]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.71      0.80      2275\n",
            "           1       0.57      0.80      0.67      1716\n",
            "           2       0.71      0.53      0.61       856\n",
            "\n",
            "    accuracy                           0.71      4847\n",
            "   macro avg       0.73      0.68      0.69      4847\n",
            "weighted avg       0.75      0.71      0.72      4847\n",
            "\n",
            "BayesianRidge\n",
            "Start tuning\n",
            "Fitting 10 folds for each of 64 candidates, totalling 640 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:    6.2s\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:   26.6s\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hw3tm3XaJA3p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Stacking\n",
        "estimators = [('bt', BT.reg), ('lgb', lg.reg), ('gb', gb.reg)]\n",
        "reg = ensemble.StackingRegressor(estimators=estimators, final_estimator=rf.reg, n_jobs = -1, passthrough = True)\n",
        "stack = Regressor(reg)\n",
        "y = stack.run(X_train, y_train, X_test, y_test)\n",
        "# with open ('y_pred_regs.pkl', 'rb') as f:\n",
        "# \ty_preds = pickle.load(f)\n",
        "with open ('regs.pkl', 'rb') as f:\n",
        "\tregs = pickle.load(f)\n",
        "\n",
        "y_preds.append(y)\n",
        "regs.append(y)\n",
        "val = input(\"Enter to save, Ctrl+C to stop\") \n",
        "# with open ('y_pred_regs.pkl', 'wb') as f:\n",
        "# \tpickle.dump(y_preds, f)\n",
        "with open ('regs.pkl', 'wb') as f:\n",
        "\tpickle.dump(regs, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSuenLnnIOLa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"---------Averaging Blending Regressor---------\")\n",
        "\n",
        "with open ('regs.pkl', 'rb') as f:\n",
        "\tregs = pickle.load(f)\n",
        "y_pred_vals = [reg.predict(X_val) for reg in regs]\n",
        "weights = [0.5, 0, 0, 0, 0.5, 0]\n",
        "y_pred_val = sum(x*y for x, y in zip(weights, y_pred_vals))\n",
        "clf_val = [map_to_class(y) for y in y_pred_val]\n",
        "print(metrics.confusion_matrix(y_val_clf, clf_val))\n",
        "print(metrics.classification_report(y_val_clf, clf_val))\n",
        "\n",
        "y_pred_tests = [reg.predict(X_test) for reg in regs]\n",
        "weights = [0.5, 0, 0, 0, 0.5, 0]\n",
        "y_pred_test = sum(x*y for x, y in zip(weights, y_pred_tests))\n",
        "clf_test = [map_to_class(y) for y in y_pred_test]\n",
        "print(metrics.confusion_matrix(y_test_clf, clf_test))\n",
        "print(metrics.classification_report(y_test_clf, clf_test))\n",
        "\n",
        "y_pred_tests = [reg.predict(X_test) for reg in regs]\n",
        "weights = [0.5, 0, 0, 0, 0.5, 0]\n",
        "y_pred_test = sum(x*y for x, y in zip(weights, y_pred_tests))\n",
        "clf_test = [map_to_class(y) for y in y_pred_test]\n",
        "print(metrics.confusion_matrix(y_test_clf, clf_test))\n",
        "print(metrics.classification_report(y_test_clf, clf_test))\n",
        "print(\"---------Blending Regressor---------\")\n",
        "X_val = np.append(X_val, np.asarray(y_pred_vals).T, axis = 1)\n",
        "X_test = np.append(X_test, np.asarray(y_pred_tests).T, axis = 1)\n",
        "model = Regressor(linear_model.LinearRegression())\n",
        "model.run(X_val, y_val, X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjSY7sdRIQqN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open ('regs.pkl', 'rb') as f:\n",
        "\tregs = pickle.load(f)\n",
        "tree_model = regs[0]\n",
        "base_model = regs[2]\n",
        "X_lr_train = tree_model.reg.apply(X_train)\n",
        "X_lr_test = tree_model.reg.apply(X_test)\n",
        "X_train = np.append(X_train, X_lr_train, axis=1)\n",
        "X_test = np.append(X_test, X_lr_test, axis=1)\n",
        "X_train = X_lr_train\n",
        "X_test = X_lr_test\n",
        "\n",
        "base_model.run(X_lr_train, y_train, X_lr_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}